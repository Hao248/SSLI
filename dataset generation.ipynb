{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faabd349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# headers\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage.io as io\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a46003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_clear(qa_value):\n",
    "    Cirrus   = (qa_value>>0) & 1\n",
    "    Cloud    = (qa_value>>1) & 1\n",
    "    Adjacent = (qa_value>>2) & 1\n",
    "    Shadow   = (qa_value>>3) & 1\n",
    "    Snow     = (qa_value>>4) & 1\n",
    "    is_clear = ( (Cloud==0) & (Cirrus==0) & (Adjacent==0) & (Shadow==0) & (Snow==0))\n",
    "    return is_clear\n",
    "def date_to_string(target):\n",
    "    if target < 10: \n",
    "        target_str = f'00{target}'\n",
    "    elif target <100:\n",
    "        target_str = f'0{target}'\n",
    "    else: target_str = str(target)\n",
    "    if target <= 0: \n",
    "        print('Receiving date 0')\n",
    "        return '000'\n",
    "    return target_str\n",
    "def search_pair(target, path_L, date_L, path_S, date_S, radius=1):\n",
    "    L_dates = []\n",
    "    S_dates = []\n",
    "    for date in range(target-radius, target+radius+1):\n",
    "        if date in date_L: L_dates.insert(len(L_dates), date)\n",
    "        if date in date_S: S_dates.insert(len(S_dates), date)\n",
    "    return L_dates, S_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee6e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_HLS(dates, year, scale, target_dir, start):\n",
    "    path_L = os.listdir(f'../HLS/dataset/L30/{year}/{target_dir}')\n",
    "    path_L.sort()\n",
    "    path_L = path_L\n",
    "    date_L = [int(name[19:22]) for name in path_L]\n",
    "    path_S = os.listdir(f'../HLS/dataset/S30/{year}/{target_dir}')\n",
    "    path_S.sort()\n",
    "    path_S = path_S\n",
    "    date_S = [int(name[19:22]) for name in path_S]\n",
    "    L_head = path_L[0][:19]\n",
    "    L_tail = path_L[0][22:]\n",
    "    S_head = path_S[0][:19]\n",
    "    S_tail = path_S[0][22:]\n",
    "    L_bands = ['B01', 'B02','B03','B04','B05','B06','B07','Fmask']\n",
    "    S_bands = ['B01', 'B02','B03','B04','B8A','B11','B12','Fmask']\n",
    "\n",
    "    fuse_cube = np.ones((dates.shape[0], 3660//scale,3660//scale,9), dtype=np.int32) * -9999\n",
    "    fuse_cube[:,:,:,8] = 0\n",
    "    for d_i, target_date in enumerate(tqdm(dates)):\n",
    "        L_dates, S_dates = search_pair(target_date, path_L, date_L, path_S, date_S)\n",
    "        total_dates = len(L_dates)+len(S_dates)\n",
    "        if total_dates == 0: continue\n",
    "        else: base_img = np.empty((total_dates,3660//scale,3660//scale,9), dtype=np.int32)\n",
    "        l_i = 0\n",
    "        if len(L_dates) > 0:\n",
    "            for l_i, d in enumerate(L_dates):\n",
    "                folder_head = f'{L_head}{date_to_string(d)}'\n",
    "                for p in path_L: \n",
    "                    if re.findall(f'{folder_head}.', p): folder_name=p\n",
    "                for b_i, b in enumerate(L_bands):\n",
    "                    base_img[l_i, :, :, b_i] = io.imread(f'../HLS/dataset/L30/{year}/{target_dir}/{folder_name}/{folder_name}.{b}.tif')[[i for i in range(start, 3660, scale)]][:,[i for i in range(start, 3660, scale)]]\n",
    "            l_i += 1\n",
    "        if len(S_dates) > 0:\n",
    "            for s_i, d in enumerate(S_dates):\n",
    "                folder_head = f'{S_head}{date_to_string(d)}'\n",
    "                for p in path_S: \n",
    "                    if re.findall(f'{folder_head}.', p): folder_name=p\n",
    "                for b_i, b in enumerate(S_bands):\n",
    "                    base_img[s_i+l_i, :, :, b_i] = io.imread(f'../HLS/dataset/S30/{year}/{target_dir}/{folder_name}/{folder_name}.{b}.tif')[[i for i in range(start, 3660, scale)]][:,[i for i in range(start, 3660, scale)]]\n",
    "        base_img[:,:,:,8] = is_clear(base_img[:,:,:,7].astype(int)) # 1 for clear, 0 for NOT clear or missing\n",
    "        missing_map = (1-base_img[:,:,:,8]).astype(bool)\n",
    "        base_img[missing_map,:7] = -9999\n",
    "        if total_dates == 1:\n",
    "            if len(L_dates) == 1:\n",
    "                fuse_cube[d_i] = base_img[0]\n",
    "            else:\n",
    "                fuse_cube[d_i] = base_img[0]\n",
    "        elif total_dates > 1:\n",
    "            fuse_img = np.ones((3660//scale,3660//scale,9), dtype=np.int32) * -9999\n",
    "            fuse_img[:,:,8] = 0\n",
    "            date_bin = base_img[:,:,:,8].sum(axis=0)\n",
    "            pix_bin = base_img[:,:,:,8].reshape(total_dates,-1).astype(int).sum(axis=1)\n",
    "            sort_date = np.argsort(pix_bin)\n",
    "            d1_maps = ((date_bin == 1) * base_img[:,:,:,8]).astype(bool)\n",
    "            for t in range(total_dates):\n",
    "                fuse_img[d1_maps[t]] = base_img[t,d1_maps[t]]\n",
    "            dmore_maps = date_bin > 1\n",
    "            for t in sort_date:\n",
    "                if base_img[t,dmore_maps,8].sum()>0:\n",
    "                    dmore_t_map = (base_img[t,:,:,8] == 1) * dmore_maps\n",
    "                    fuse_img[dmore_t_map] = base_img[t, dmore_t_map]\n",
    "            fuse_cube[d_i] = fuse_img\n",
    "        else:\n",
    "            print('error in dates')\n",
    "            break\n",
    "        fuse_cube[:,:,:,7] = fuse_cube[:,:,:,8]\n",
    "        total_n = fuse_cube[:,:,:,-1].reshape(122,-1).T.sum(axis=1)\n",
    "    return fuse_cube[:,:,:,:8], total_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6103ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date, end_date, date_radius = 1,365,1\n",
    "dates = np.arange(start_date,end_date+1,date_radius*2+1)\n",
    "train_tiles = ['11/S/P/R','12/T/V/K','14/T/N/P','17/R/N/J','18/T/W/Q']\n",
    "valid_tiles = ['11/S/P/S','12/T/V/L','14/T/P/P','17/R/N/K','18/T/W/P']\n",
    "test_tiles = ['11/S/Q/T', '12/T/V/M', '14/T/Q/P', '17/R/N/L', '18/T/W/N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a76d0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for i in range(122):\n",
    "    lst.append(f'{i:03d}.coastal_blue')\n",
    "    lst.append(f'{i:03d}.blue')\n",
    "    lst.append(f'{i:03d}.green')\n",
    "    lst.append(f'{i:03d}.red')\n",
    "    lst.append(f'{i:03d}.nir')\n",
    "    lst.append(f'{i:03d}.swir1')\n",
    "    lst.append(f'{i:03d}.swir2')\n",
    "    lst.append(f'{i:03d}.qa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a09e57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_val = ['col','row','image_year']+lst+['total_n']\n",
    "width = len(col_val) # tile_id and TVT\n",
    "scale = 10\n",
    "divide = 5\n",
    "height = 3660*3660//scale//scale*2*15\n",
    "total_matrix = np.zeros((height, width), dtype=np.int32)\n",
    "year = 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebad5650",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in trange(5):\n",
    "    train_cube_cr = np.indices((3660,3660)).transpose(1,2,0)[[i for i in range(divide, 3660, scale)]][:,[i for i in range(divide, 3660, scale)]].reshape(-1,2)\n",
    "    vt_cube_cr = np.indices((3660,3660)).transpose(1,2,0)[[i for i in range(divide, 3660, scale)]][:,[i for i in range(divide, 3660, scale)]].reshape(-1,2)\n",
    "    train_cube1, train1_total_n = fuse_HLS(dates, year, scale, train_tiles[j], divide)\n",
    "    val_cube1, val1_total_n = fuse_HLS(dates, year, scale, train_tiles[j], 0)\n",
    "    train_cube2, train2_total_n = fuse_HLS(dates, year, scale, valid_tiles[j], divide)\n",
    "    val_cube2, val2_total_n = fuse_HLS(dates, year, scale, valid_tiles[j], 0)\n",
    "    train_cube3, train3_total_n = fuse_HLS(dates, year, scale, test_tiles[j], divide)\n",
    "    test_cube, test_total_n = fuse_HLS(dates, year, scale, test_tiles[j], 0)\n",
    "    \n",
    "    # col & row\n",
    "    total_matrix[(j*6+0)*3660*3660//scale//scale: (j*6+3)*3660*3660//scale//scale, 0:2] = np.concatenate((train_cube_cr,train_cube_cr,train_cube_cr))\n",
    "    total_matrix[(j*6+3)*3660*3660//scale//scale: (j*6+6)*3660*3660//scale//scale, 0:2] = np.concatenate((vt_cube_cr,vt_cube_cr,train_cube_cr))\n",
    "    # img_yr\n",
    "    total_matrix[(j*6+0)*3660*3660//scale//scale: (j*6+6)*3660*3660//scale//scale, 2] = year\n",
    "    # band\n",
    "    total_matrix[(j*6+0)*3660*3660//scale//scale: (j*6+1)*3660*3660//scale//scale, 3:-1] = train_cube1.transpose(1,2,0,3).reshape(-1,8*122)\n",
    "    total_matrix[(j*6+1)*3660*3660//scale//scale: (j*6+2)*3660*3660//scale//scale, 3:-1] = train_cube2.transpose(1,2,0,3).reshape(-1,8*122)\n",
    "    total_matrix[(j*6+2)*3660*3660//scale//scale: (j*6+3)*3660*3660//scale//scale, 3:-1] = train_cube3.transpose(1,2,0,3).reshape(-1,8*122)\n",
    "    total_matrix[(j*6+3)*3660*3660//scale//scale: (j*6+4)*3660*3660//scale//scale, 3:-1] = val_cube1.transpose(1,2,0,3).reshape(-1,8*122)\n",
    "    total_matrix[(j*6+4)*3660*3660//scale//scale: (j*6+5)*3660*3660//scale//scale, 3:-1] = val_cube2.transpose(1,2,0,3).reshape(-1,8*122)\n",
    "    total_matrix[(j*6+5)*3660*3660//scale//scale: (j*6+6)*3660*3660//scale//scale, 3:-1] = test_cube.transpose(1,2,0,3).reshape(-1,8*122)\n",
    "    # total_n\n",
    "    total_matrix[(j*6+0)*3660*3660//scale//scale: (j*6+1)*3660*3660//scale//scale, -1] = train1_total_n\n",
    "    total_matrix[(j*6+1)*3660*3660//scale//scale: (j*6+2)*3660*3660//scale//scale, -1] = train2_total_n\n",
    "    total_matrix[(j*6+2)*3660*3660//scale//scale: (j*6+3)*3660*3660//scale//scale, -1] = train3_total_n\n",
    "    total_matrix[(j*6+3)*3660*3660//scale//scale: (j*6+4)*3660*3660//scale//scale, -1] = val1_total_n\n",
    "    total_matrix[(j*6+4)*3660*3660//scale//scale: (j*6+5)*3660*3660//scale//scale, -1] = val2_total_n\n",
    "    total_matrix[(j*6+5)*3660*3660//scale//scale: (j*6+6)*3660*3660//scale//scale, -1] = test_total_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2667d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(total_matrix, columns=col_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8257ae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tile_id'] = pd.Series(dtype='string')\n",
    "df['TVT'] = pd.Series(dtype='string')\n",
    "df['lat'] = pd.Series(dtype='float32')\n",
    "df['lon'] = pd.Series(dtype='float32')\n",
    "for i in trange(5):\n",
    "    # tile_id\n",
    "    df.loc[(i*6+0)*3660*3660//scale//scale: (i*6+1)*3660*3660//scale//scale,'tile_id'] = f'{train_tiles[i][:2]+train_tiles[i][3]+train_tiles[i][5]+train_tiles[i][7]}'\n",
    "    df.loc[(i*6+1)*3660*3660//scale//scale: (i*6+2)*3660*3660//scale//scale,'tile_id'] = f'{valid_tiles[i][:2]+valid_tiles[i][3]+valid_tiles[i][5]+valid_tiles[i][7]}'\n",
    "    df.loc[(i*6+2)*3660*3660//scale//scale: (i*6+3)*3660*3660//scale//scale,'tile_id'] = f'{test_tiles[i][:2]+test_tiles[i][3]+test_tiles[i][5]+test_tiles[i][7]}'\n",
    "    df.loc[(i*6+3)*3660*3660//scale//scale: (i*6+4)*3660*3660//scale//scale,'tile_id'] = f'{train_tiles[i][:2]+train_tiles[i][3]+train_tiles[i][5]+train_tiles[i][7]}'\n",
    "    df.loc[(i*6+4)*3660*3660//scale//scale: (i*6+5)*3660*3660//scale//scale,'tile_id'] = f'{valid_tiles[i][:2]+valid_tiles[i][3]+valid_tiles[i][5]+valid_tiles[i][7]}'\n",
    "    df.loc[(i*6+5)*3660*3660//scale//scale: (i*6+6)*3660*3660//scale//scale,'tile_id'] = f'{test_tiles[i][:2]+test_tiles[i][3]+test_tiles[i][5]+test_tiles[i][7]}'\n",
    "    \n",
    "    # TVT\n",
    "    df.loc[(i*6+0)*3660*3660//scale//scale: (i*6+1)*3660*3660//scale//scale,'TVT'] = f'train'\n",
    "    df.loc[(i*6+1)*3660*3660//scale//scale: (i*6+2)*3660*3660//scale//scale,'TVT'] = f'train'\n",
    "    df.loc[(i*6+2)*3660*3660//scale//scale: (i*6+3)*3660*3660//scale//scale,'TVT'] = f'train'\n",
    "    df.loc[(i*6+3)*3660*3660//scale//scale: (i*6+4)*3660*3660//scale//scale,'TVT'] = f'valid'\n",
    "    df.loc[(i*6+4)*3660*3660//scale//scale: (i*6+5)*3660*3660//scale//scale,'TVT'] = f'valid'\n",
    "    df.loc[(i*6+5)*3660*3660//scale//scale: (i*6+6)*3660*3660//scale//scale,'TVT'] = f'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c73d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'F:/dataset.csv', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d147b3be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai1",
   "language": "python",
   "name": "fastai1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
